{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mipeichao/kaggle-bards-play-2?scriptVersionId=258440627\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"a3cba625","metadata":{"papermill":{"duration":0.006848,"end_time":"2025-08-27T08:42:29.83231","exception":false,"start_time":"2025-08-27T08:42:29.825462","status":"completed"},"tags":[]},"source":["This program uses the Tiny Shakespeare dataset to explore data preprocessing techniques. At the end, I include two discussions inspired by Google AI:<br>\n","\n","Should tokenization of an NLP dataset be done row by row?<br>\n","\n","Should punctuation be handled before or after tokenization?<br>\n","\n","In addition, how can I effectively remove the names in the Shakespeare dataset? Today's program uses en_core_web_sm, but it didn't work. I have decided to find a character name list from Bard's plays and use it for comparison to remove the names.\n","\n"]},{"cell_type":"markdown","id":"f7263bd7","metadata":{"papermill":{"duration":0.005617,"end_time":"2025-08-27T08:42:29.84415","exception":false,"start_time":"2025-08-27T08:42:29.838533","status":"completed"},"tags":[]},"source":["References:<br>\n","[All  Engineering  Machine Learning\n","What are the most effective ways to train and test NLP models on different datasets?](https://www.linkedin.com/advice/3/what-most-effective-ways-train-test-nlp-models-rb3ke)<br>\n","[Key Steps in Text Preprocessing + Hands-on with Python](https://pub.towardsai.net/text-preprocessing-for-nlp-a-step-by-step-guide-to-clean-raw-text-data-2bb8918a4e2c)<br>\n","[Do we need to pre-process both the test and train data set?\n","](https://datascience.stackexchange.com/questions/103211/do-we-need-to-pre-process-both-the-test-and-train-data-set)<br>\n","[Coding tips](https://www.cs.williams.edu/~kkeith/teaching/s23/cs375/attach/shakespeare-gen.html)"]},{"cell_type":"markdown","id":"5330c18e","metadata":{"papermill":{"duration":0.005685,"end_time":"2025-08-27T08:42:29.855872","exception":false,"start_time":"2025-08-27T08:42:29.850187","status":"completed"},"tags":[]},"source":["Tokenization: Split text into individual words or tokens.<br>\n","Lowercasing: Convert all text to lowercase to ensure consistency.<br>\n","Removing Noise: Eliminate irrelevant characters, punctuation, or special symbols.<br>\n","Stopword Removal: Exclude common words like \"and\", \"the\", etc., which don't contribute much to meaning.<br>\n","Stemming or Lemmatization: Reduce words to their root form to normalize variations (e.g., \"running\" to \"run\").<br>\n","Handling Numerical Data: Convert numbers to a standard format if necessary.<br>\n","Handling Rare Words: Replace rare or misspelled words with a special token or correct them.<br>\n","Padding or Truncation: Make all sequences uniform in length by adding padding or truncating.<br>"]},{"cell_type":"code","execution_count":1,"id":"607048a5","metadata":{"execution":{"iopub.execute_input":"2025-08-27T08:42:29.869187Z","iopub.status.busy":"2025-08-27T08:42:29.868861Z","iopub.status.idle":"2025-08-27T08:42:30.56174Z","shell.execute_reply":"2025-08-27T08:42:30.560611Z"},"papermill":{"duration":0.701702,"end_time":"2025-08-27T08:42:30.563518","exception":false,"start_time":"2025-08-27T08:42:29.861816","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Path to dataset files: /kaggle/input/the-bards-best-a-character-modeling-dataset\n"]}],"source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"thedevastator/the-bards-best-a-character-modeling-dataset\")\n","\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","execution_count":2,"id":"a0e1a68a","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:56:38.03617Z","start_time":"2025-08-27T07:56:37.693539Z"},"execution":{"iopub.execute_input":"2025-08-27T08:42:30.577298Z","iopub.status.busy":"2025-08-27T08:42:30.576964Z","iopub.status.idle":"2025-08-27T08:42:32.682592Z","shell.execute_reply":"2025-08-27T08:42:32.681547Z"},"papermill":{"duration":2.114252,"end_time":"2025-08-27T08:42:32.684308","exception":false,"start_time":"2025-08-27T08:42:30.570056","status":"completed"},"tags":[]},"outputs":[],"source":["import pandas as pd\n","import re"]},{"cell_type":"code","execution_count":3,"id":"01fae622","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:56:38.046942Z","start_time":"2025-08-27T07:56:38.042385Z"},"execution":{"iopub.execute_input":"2025-08-27T08:42:32.698755Z","iopub.status.busy":"2025-08-27T08:42:32.698223Z","iopub.status.idle":"2025-08-27T08:42:32.704932Z","shell.execute_reply":"2025-08-27T08:42:32.704014Z"},"papermill":{"duration":0.015266,"end_time":"2025-08-27T08:42:32.706517","exception":false,"start_time":"2025-08-27T08:42:32.691251","status":"completed"},"tags":[]},"outputs":[],"source":["# Read the origin data and create DataFrame\n","def load_data(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        content = f.read()\n","    lines = re.sub(r'\\d+','', str(content))   # Remove digits\n","    lines = re.sub(r'[^\\w\\s]','', str(lines))   # Remove punctuation\n","    lines = lines.replace('\\r\\n', '\\n').replace('\\r', '\\n').split('\\n')  # Normalize newlines\n","    lines = [line.strip() for line in lines if line.strip()]  # Remove empty lines\n","    return pd.DataFrame({'origin_text': lines})"]},{"cell_type":"markdown","id":"3f6058d0","metadata":{"papermill":{"duration":0.005678,"end_time":"2025-08-27T08:42:32.718294","exception":false,"start_time":"2025-08-27T08:42:32.712616","status":"completed"},"tags":[]},"source":["`f` is a file object created by the `open()` function. It represents the opened file specified by `file_path`, allowing you to read its contents using methods like `f.read()`."]},{"cell_type":"markdown","id":"086b9067","metadata":{"papermill":{"duration":0.006408,"end_time":"2025-08-27T08:42:32.731796","exception":false,"start_time":"2025-08-27T08:42:32.725388","status":"completed"},"tags":[]},"source":["This line creates a new list by iterating over each string in lines, removing leading and trailing whitespace with strip(), and including only non-empty lines. It effectively cleans up the list by removing blank or whitespace-only lines."]},{"cell_type":"code","execution_count":4,"id":"323e02ca","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:56:38.105982Z","start_time":"2025-08-27T07:56:38.052571Z"},"execution":{"iopub.execute_input":"2025-08-27T08:42:32.747272Z","iopub.status.busy":"2025-08-27T08:42:32.746906Z","iopub.status.idle":"2025-08-27T08:42:32.936116Z","shell.execute_reply":"2025-08-27T08:42:32.93521Z"},"papermill":{"duration":0.199769,"end_time":"2025-08-27T08:42:32.938201","exception":false,"start_time":"2025-08-27T08:42:32.738432","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>origin_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>text</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>First Citizen</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Before we proceed any further hear me speak</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>All</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Speak speak</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>29238</th>\n","      <td>Talk not to me I will go sit and weep</td>\n","    </tr>\n","    <tr>\n","      <th>29239</th>\n","      <td>Till I can find occasion of revenge</td>\n","    </tr>\n","    <tr>\n","      <th>29240</th>\n","      <td>BAPTISTA</td>\n","    </tr>\n","    <tr>\n","      <th>29241</th>\n","      <td>Was ever gentleman thus grieved as I</td>\n","    </tr>\n","    <tr>\n","      <th>29242</th>\n","      <td>But who comes here</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>29243 rows × 1 columns</p>\n","</div>"],"text/plain":["                                       origin_text\n","0                                             text\n","1                                    First Citizen\n","2      Before we proceed any further hear me speak\n","3                                              All\n","4                                      Speak speak\n","...                                            ...\n","29238        Talk not to me I will go sit and weep\n","29239          Till I can find occasion of revenge\n","29240                                     BAPTISTA\n","29241         Was ever gentleman thus grieved as I\n","29242                           But who comes here\n","\n","[29243 rows x 1 columns]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>origin_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>text</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>rance taen</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>As shall with either parts agreement stand</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BAPTISTA</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Not in my house Lucentio for you know</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1830</th>\n","      <td>And yet so fast asleep</td>\n","    </tr>\n","    <tr>\n","      <th>1831</th>\n","      <td>ANTONIO</td>\n","    </tr>\n","    <tr>\n","      <th>1832</th>\n","      <td>Noble Sebastian</td>\n","    </tr>\n","    <tr>\n","      <th>1833</th>\n","      <td>Thou letst thy fortune sleepdie rather winkst</td>\n","    </tr>\n","    <tr>\n","      <th>1834</th>\n","      <td>Whiles thou art waking</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1835 rows × 1 columns</p>\n","</div>"],"text/plain":["                                        origin_text\n","0                                              text\n","1                                        rance taen\n","2        As shall with either parts agreement stand\n","3                                          BAPTISTA\n","4             Not in my house Lucentio for you know\n","...                                             ...\n","1830                         And yet so fast asleep\n","1831                                        ANTONIO\n","1832                                Noble Sebastian\n","1833  Thou letst thy fortune sleepdie rather winkst\n","1834                         Whiles thou art waking\n","\n","[1835 rows x 1 columns]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>origin_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>text</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GREMIO</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Good morrow neighbour Baptista</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BAPTISTA</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Good morrow neighbour Gremio</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1698</th>\n","      <td>The match is made and all is done</td>\n","    </tr>\n","    <tr>\n","      <th>1699</th>\n","      <td>Your son shall have my daughter with consent</td>\n","    </tr>\n","    <tr>\n","      <th>1700</th>\n","      <td>TRANIO</td>\n","    </tr>\n","    <tr>\n","      <th>1701</th>\n","      <td>I thank you sir Where then do you know best</td>\n","    </tr>\n","    <tr>\n","      <th>1702</th>\n","      <td>We be affied and such assu</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1703 rows × 1 columns</p>\n","</div>"],"text/plain":["                                       origin_text\n","0                                             text\n","1                                           GREMIO\n","2                   Good morrow neighbour Baptista\n","3                                         BAPTISTA\n","4                     Good morrow neighbour Gremio\n","...                                            ...\n","1698             The match is made and all is done\n","1699  Your son shall have my daughter with consent\n","1700                                        TRANIO\n","1701   I thank you sir Where then do you know best\n","1702                    We be affied and such assu\n","\n","[1703 rows x 1 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["df_train = load_data('/kaggle/input/the-bards-best-a-character-modeling-dataset/train.csv')\n","df_test = load_data('/kaggle/input/the-bards-best-a-character-modeling-dataset/test.csv')\n","df_validation = load_data('/kaggle/input/the-bards-best-a-character-modeling-dataset/validation.csv')\n","\n","from IPython.display import display\n","\n","display(df_train)\n","display(df_test)\n","display(df_validation)"]},{"cell_type":"code","execution_count":5,"id":"d4508f85","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:56:38.220323Z","start_time":"2025-08-27T07:56:38.217028Z"},"execution":{"iopub.execute_input":"2025-08-27T08:42:32.953258Z","iopub.status.busy":"2025-08-27T08:42:32.952965Z","iopub.status.idle":"2025-08-27T08:42:32.9583Z","shell.execute_reply":"2025-08-27T08:42:32.957165Z"},"papermill":{"duration":0.014766,"end_time":"2025-08-27T08:42:32.96006","exception":false,"start_time":"2025-08-27T08:42:32.945294","status":"completed"},"tags":[]},"outputs":[],"source":["def remove_characters(texts) :\n","    out = [e for e in texts if e[-1]!=\":\" and e!=\"text\"]\n","    return pd.DataFrame({'origin_text': out})"]},{"cell_type":"code","execution_count":6,"id":"d07c8d67","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:56:38.303045Z","start_time":"2025-08-27T07:56:38.28671Z"},"execution":{"iopub.execute_input":"2025-08-27T08:42:32.974879Z","iopub.status.busy":"2025-08-27T08:42:32.974531Z","iopub.status.idle":"2025-08-27T08:42:32.996625Z","shell.execute_reply":"2025-08-27T08:42:32.995529Z"},"papermill":{"duration":0.03188,"end_time":"2025-08-27T08:42:32.998456","exception":false,"start_time":"2025-08-27T08:42:32.966576","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>origin_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>First Citizen</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Before we proceed any further hear me speak</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>All</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Speak speak</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>First Citizen</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>29237</th>\n","      <td>Talk not to me I will go sit and weep</td>\n","    </tr>\n","    <tr>\n","      <th>29238</th>\n","      <td>Till I can find occasion of revenge</td>\n","    </tr>\n","    <tr>\n","      <th>29239</th>\n","      <td>BAPTISTA</td>\n","    </tr>\n","    <tr>\n","      <th>29240</th>\n","      <td>Was ever gentleman thus grieved as I</td>\n","    </tr>\n","    <tr>\n","      <th>29241</th>\n","      <td>But who comes here</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>29242 rows × 1 columns</p>\n","</div>"],"text/plain":["                                       origin_text\n","0                                    First Citizen\n","1      Before we proceed any further hear me speak\n","2                                              All\n","3                                      Speak speak\n","4                                    First Citizen\n","...                                            ...\n","29237        Talk not to me I will go sit and weep\n","29238          Till I can find occasion of revenge\n","29239                                     BAPTISTA\n","29240         Was ever gentleman thus grieved as I\n","29241                           But who comes here\n","\n","[29242 rows x 1 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df_train1 = remove_characters(df_train[\"origin_text\"])\n","df_test1 = remove_characters(df_test[\"origin_text\"])\n","df_validation1 = remove_characters(df_validation[\"origin_text\"])\n","\n","df_train1"]},{"cell_type":"code","execution_count":7,"id":"59ddab54","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:56:39.530641Z","start_time":"2025-08-27T07:56:38.343416Z"},"execution":{"iopub.execute_input":"2025-08-27T08:42:33.013374Z","iopub.status.busy":"2025-08-27T08:42:33.013061Z","iopub.status.idle":"2025-08-27T08:42:45.382044Z","shell.execute_reply":"2025-08-27T08:42:45.380972Z"},"papermill":{"duration":12.378689,"end_time":"2025-08-27T08:42:45.383891","exception":false,"start_time":"2025-08-27T08:42:33.005202","status":"completed"},"tags":[]},"outputs":[],"source":["# Batch processing with nlp.pipe for speed\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def remove_named_entities_batch(texts):\n","    cleaned_texts = []\n","    for doc in nlp.pipe(texts, batch_size=32):\n","        cleaned = doc.text\n","        for ent in doc.ents:\n","            if ent.label_ == \"PERSON\":\n","                cleaned = cleaned.replace(ent.text, \"\")\n","        cleaned_texts.append(cleaned)\n","    return cleaned_texts"]},{"cell_type":"markdown","id":"49d6a1e2","metadata":{"papermill":{"duration":0.006284,"end_time":"2025-08-27T08:42:45.396896","exception":false,"start_time":"2025-08-27T08:42:45.390612","status":"completed"},"tags":[]},"source":["This function removes named entities labeled as \"PERSON\" from a list of texts using spaCy for efficient batch processing.\n","\n","- It uses `nlp.pipe()` to process all texts in batches (faster than processing one by one).\n","- For each document, it replaces any detected person name with `[PERSON]`.\n","- The cleaned texts are collected and returned as a list.\n","\n","This approach is efficient for large datasets and anonymizes person names in the text."]},{"cell_type":"markdown","id":"2f5087d5","metadata":{"papermill":{"duration":0.006154,"end_time":"2025-08-27T08:42:45.409384","exception":false,"start_time":"2025-08-27T08:42:45.40323","status":"completed"},"tags":[]},"source":["`doc.ents` is a spaCy property that returns a list of named entities detected in a processed text (a `Doc` object). Each entity has attributes like `text` (the entity string) and `label_` (the entity type, e.g., PERSON, ORG).\n","\n","Example:\n","```python\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(\"Barack Obama was born in Hawaii.\")\n","for ent in doc.ents:\n","    print(ent.text, ent.label_)\n","# Output: Barack Obama PERSON, Hawaii GPE\n","```\n","So, `doc.ents` gives you all recognized entities in the text."]},{"cell_type":"code","execution_count":8,"id":"0aacb102","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:05.896152Z","start_time":"2025-08-27T07:56:39.646132Z"},"execution":{"iopub.execute_input":"2025-08-27T08:42:45.423891Z","iopub.status.busy":"2025-08-27T08:42:45.423149Z","iopub.status.idle":"2025-08-27T08:43:34.475957Z","shell.execute_reply":"2025-08-27T08:43:34.47498Z"},"papermill":{"duration":49.067607,"end_time":"2025-08-27T08:43:34.483348","exception":false,"start_time":"2025-08-27T08:42:45.415741","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>origin_text</th>\n","      <th>Cleaned_Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>First Citizen</td>\n","      <td>First Citizen</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Before we proceed any further hear me speak</td>\n","      <td>Before we proceed any further hear me speak</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>All</td>\n","      <td>All</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Speak speak</td>\n","      <td>Speak speak</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>First Citizen</td>\n","      <td>First Citizen</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>29237</th>\n","      <td>Talk not to me I will go sit and weep</td>\n","      <td>Talk not to me I will go sit and weep</td>\n","    </tr>\n","    <tr>\n","      <th>29238</th>\n","      <td>Till I can find occasion of revenge</td>\n","      <td>Till I can find occasion of revenge</td>\n","    </tr>\n","    <tr>\n","      <th>29239</th>\n","      <td>BAPTISTA</td>\n","      <td>BAPTISTA</td>\n","    </tr>\n","    <tr>\n","      <th>29240</th>\n","      <td>Was ever gentleman thus grieved as I</td>\n","      <td>Was ever gentleman thus grieved as I</td>\n","    </tr>\n","    <tr>\n","      <th>29241</th>\n","      <td>But who comes here</td>\n","      <td>But who comes here</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>29242 rows × 2 columns</p>\n","</div>"],"text/plain":["                                       origin_text  \\\n","0                                    First Citizen   \n","1      Before we proceed any further hear me speak   \n","2                                              All   \n","3                                      Speak speak   \n","4                                    First Citizen   \n","...                                            ...   \n","29237        Talk not to me I will go sit and weep   \n","29238          Till I can find occasion of revenge   \n","29239                                     BAPTISTA   \n","29240         Was ever gentleman thus grieved as I   \n","29241                           But who comes here   \n","\n","                                      Cleaned_Text  \n","0                                    First Citizen  \n","1      Before we proceed any further hear me speak  \n","2                                              All  \n","3                                      Speak speak  \n","4                                    First Citizen  \n","...                                            ...  \n","29237        Talk not to me I will go sit and weep  \n","29238          Till I can find occasion of revenge  \n","29239                                     BAPTISTA  \n","29240         Was ever gentleman thus grieved as I  \n","29241                           But who comes here  \n","\n","[29242 rows x 2 columns]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["df_train1['Cleaned_Text'] = remove_named_entities_batch(df_train1['origin_text'])\n","\n","df_train1"]},{"cell_type":"markdown","id":"a19b8644","metadata":{"papermill":{"duration":0.00665,"end_time":"2025-08-27T08:43:34.496807","exception":false,"start_time":"2025-08-27T08:43:34.490157","status":"completed"},"tags":[]},"source":["The unique values 21461 and 21454 shown in your DataFrame's `describe()` output represent the number of unique rows (sentences or text entries), **not** characters or words. Each row is typically a sentence or document, so these numbers count unique text entries in your dataset."]},{"cell_type":"code","execution_count":9,"id":"418293f2","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:07.606491Z","start_time":"2025-08-27T07:57:06.160117Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:34.512003Z","iopub.status.busy":"2025-08-27T08:43:34.511598Z","iopub.status.idle":"2025-08-27T08:43:37.329069Z","shell.execute_reply":"2025-08-27T08:43:37.327789Z"},"papermill":{"duration":2.827386,"end_time":"2025-08-27T08:43:37.330915","exception":false,"start_time":"2025-08-27T08:43:34.503529","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>origin_text</th>\n","      <th>Cleaned_Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>rance taen</td>\n","      <td>rance taen</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>As shall with either parts agreement stand</td>\n","      <td>As shall with either parts agreement stand</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>BAPTISTA</td>\n","      <td>BAPTISTA</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Not in my house Lucentio for you know</td>\n","      <td>Not in my house  for you know</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Pitchers have ears and I have many servants</td>\n","      <td>Pitchers have ears and I have many servants</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1829</th>\n","      <td>And yet so fast asleep</td>\n","      <td>And yet so fast asleep</td>\n","    </tr>\n","    <tr>\n","      <th>1830</th>\n","      <td>ANTONIO</td>\n","      <td>ANTONIO</td>\n","    </tr>\n","    <tr>\n","      <th>1831</th>\n","      <td>Noble Sebastian</td>\n","      <td>Noble Sebastian</td>\n","    </tr>\n","    <tr>\n","      <th>1832</th>\n","      <td>Thou letst thy fortune sleepdie rather winkst</td>\n","      <td>Thou letst thy fortune sleepdie rather winkst</td>\n","    </tr>\n","    <tr>\n","      <th>1833</th>\n","      <td>Whiles thou art waking</td>\n","      <td>Whiles thou art waking</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1834 rows × 2 columns</p>\n","</div>"],"text/plain":["                                        origin_text  \\\n","0                                        rance taen   \n","1        As shall with either parts agreement stand   \n","2                                          BAPTISTA   \n","3             Not in my house Lucentio for you know   \n","4       Pitchers have ears and I have many servants   \n","...                                             ...   \n","1829                         And yet so fast asleep   \n","1830                                        ANTONIO   \n","1831                                Noble Sebastian   \n","1832  Thou letst thy fortune sleepdie rather winkst   \n","1833                         Whiles thou art waking   \n","\n","                                       Cleaned_Text  \n","0                                        rance taen  \n","1        As shall with either parts agreement stand  \n","2                                          BAPTISTA  \n","3                     Not in my house  for you know  \n","4       Pitchers have ears and I have many servants  \n","...                                             ...  \n","1829                         And yet so fast asleep  \n","1830                                        ANTONIO  \n","1831                                Noble Sebastian  \n","1832  Thou letst thy fortune sleepdie rather winkst  \n","1833                         Whiles thou art waking  \n","\n","[1834 rows x 2 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["df_test1['Cleaned_Text'] = remove_named_entities_batch(df_test1['origin_text'])\n","\n","df_test1"]},{"cell_type":"code","execution_count":10,"id":"7b67e189","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:09.197066Z","start_time":"2025-08-27T07:57:07.730761Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:37.346626Z","iopub.status.busy":"2025-08-27T08:43:37.346255Z","iopub.status.idle":"2025-08-27T08:43:40.072858Z","shell.execute_reply":"2025-08-27T08:43:40.071844Z"},"papermill":{"duration":2.736347,"end_time":"2025-08-27T08:43:40.074432","exception":false,"start_time":"2025-08-27T08:43:37.338085","status":"completed"},"tags":[]},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>origin_text</th>\n","      <th>Cleaned_Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>GREMIO</td>\n","      <td>GREMIO</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Good morrow neighbour Baptista</td>\n","      <td>Good morrow neighbour</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>BAPTISTA</td>\n","      <td>BAPTISTA</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Good morrow neighbour Gremio</td>\n","      <td>Good morrow neighbour Gremio</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>God save you gentlemen</td>\n","      <td>God save you gentlemen</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1697</th>\n","      <td>The match is made and all is done</td>\n","      <td>The match is made and all is done</td>\n","    </tr>\n","    <tr>\n","      <th>1698</th>\n","      <td>Your son shall have my daughter with consent</td>\n","      <td>Your son shall have my daughter with consent</td>\n","    </tr>\n","    <tr>\n","      <th>1699</th>\n","      <td>TRANIO</td>\n","      <td>TRANIO</td>\n","    </tr>\n","    <tr>\n","      <th>1700</th>\n","      <td>I thank you sir Where then do you know best</td>\n","      <td>I thank you sir Where then do you know best</td>\n","    </tr>\n","    <tr>\n","      <th>1701</th>\n","      <td>We be affied and such assu</td>\n","      <td>We be affied and such</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1702 rows × 2 columns</p>\n","</div>"],"text/plain":["                                       origin_text  \\\n","0                                           GREMIO   \n","1                   Good morrow neighbour Baptista   \n","2                                         BAPTISTA   \n","3                     Good morrow neighbour Gremio   \n","4                           God save you gentlemen   \n","...                                            ...   \n","1697             The match is made and all is done   \n","1698  Your son shall have my daughter with consent   \n","1699                                        TRANIO   \n","1700   I thank you sir Where then do you know best   \n","1701                    We be affied and such assu   \n","\n","                                      Cleaned_Text  \n","0                                           GREMIO  \n","1                           Good morrow neighbour   \n","2                                         BAPTISTA  \n","3                     Good morrow neighbour Gremio  \n","4                           God save you gentlemen  \n","...                                            ...  \n","1697             The match is made and all is done  \n","1698  Your son shall have my daughter with consent  \n","1699                                        TRANIO  \n","1700   I thank you sir Where then do you know best  \n","1701                        We be affied and such   \n","\n","[1702 rows x 2 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df_validation1['Cleaned_Text'] = remove_named_entities_batch(df_validation1['origin_text'])\n","\n","df_validation1"]},{"cell_type":"code","execution_count":11,"id":"59f2a2fc","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:09.37331Z","start_time":"2025-08-27T07:57:09.370182Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:40.090658Z","iopub.status.busy":"2025-08-27T08:43:40.090352Z","iopub.status.idle":"2025-08-27T08:43:40.095167Z","shell.execute_reply":"2025-08-27T08:43:40.094236Z"},"papermill":{"duration":0.014379,"end_time":"2025-08-27T08:43:40.096591","exception":false,"start_time":"2025-08-27T08:43:40.082212","status":"completed"},"tags":[]},"outputs":[],"source":["import re\n","\n","def tokenize_text(text):\n","    return re.split(r\"\\W+\", text.lower())"]},{"cell_type":"code","execution_count":12,"id":"be21e113","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:09.475991Z","start_time":"2025-08-27T07:57:09.410769Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:40.113033Z","iopub.status.busy":"2025-08-27T08:43:40.112043Z","iopub.status.idle":"2025-08-27T08:43:40.236585Z","shell.execute_reply":"2025-08-27T08:43:40.235603Z"},"papermill":{"duration":0.134357,"end_time":"2025-08-27T08:43:40.238134","exception":false,"start_time":"2025-08-27T08:43:40.103777","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0                                         [first, citizen]\n","1        [before, we, proceed, any, further, hear, me, ...\n","2                                                    [all]\n","3                                           [speak, speak]\n","4                                         [first, citizen]\n","                               ...                        \n","29237     [talk, not, to, me, i, will, go, sit, and, weep]\n","29238          [till, i, can, find, occasion, of, revenge]\n","29239                                           [baptista]\n","29240         [was, ever, gentleman, thus, grieved, as, i]\n","29241                              [but, who, comes, here]\n","Name: Cleaned_Text, Length: 29242, dtype: object"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Apply to each row in the Series\n","tokens_train = df_train1['Cleaned_Text'].apply(tokenize_text)\n","tokens_train\n"]},{"cell_type":"code","execution_count":13,"id":"e5fd905b","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:09.518812Z","start_time":"2025-08-27T07:57:09.510103Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:40.255152Z","iopub.status.busy":"2025-08-27T08:43:40.254853Z","iopub.status.idle":"2025-08-27T08:43:40.272221Z","shell.execute_reply":"2025-08-27T08:43:40.271084Z"},"papermill":{"duration":0.028027,"end_time":"2025-08-27T08:43:40.273912","exception":false,"start_time":"2025-08-27T08:43:40.245885","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0                                           [rance, taen]\n","1       [as, shall, with, either, parts, agreement, st...\n","2                                              [baptista]\n","3                    [not, in, my, house, for, you, know]\n","4       [pitchers, have, ears, and, i, have, many, ser...\n","                              ...                        \n","1829                         [and, yet, so, fast, asleep]\n","1830                                            [antonio]\n","1831                                   [noble, sebastian]\n","1832    [thou, letst, thy, fortune, sleepdie, rather, ...\n","1833                          [whiles, thou, art, waking]\n","Name: Cleaned_Text, Length: 1834, dtype: object"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["tokens_test = df_test1['Cleaned_Text'].apply(tokenize_text)\n","tokens_test"]},{"cell_type":"code","execution_count":14,"id":"adfc6d4e","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:09.761419Z","start_time":"2025-08-27T07:57:09.752248Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:40.290646Z","iopub.status.busy":"2025-08-27T08:43:40.290305Z","iopub.status.idle":"2025-08-27T08:43:40.306186Z","shell.execute_reply":"2025-08-27T08:43:40.304897Z"},"papermill":{"duration":0.025988,"end_time":"2025-08-27T08:43:40.307726","exception":false,"start_time":"2025-08-27T08:43:40.281738","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0                                                [gremio]\n","1                             [good, morrow, neighbour, ]\n","2                                              [baptista]\n","3                       [good, morrow, neighbour, gremio]\n","4                             [god, save, you, gentlemen]\n","                              ...                        \n","1697           [the, match, is, made, and, all, is, done]\n","1698    [your, son, shall, have, my, daughter, with, c...\n","1699                                             [tranio]\n","1700    [i, thank, you, sir, where, then, do, you, kno...\n","1701                        [we, be, affied, and, such, ]\n","Name: Cleaned_Text, Length: 1702, dtype: object"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["tokens_validation = df_validation1['Cleaned_Text'].apply(tokenize_text)\n","tokens_validation"]},{"cell_type":"code","execution_count":15,"id":"cc0226de","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:10.746217Z","start_time":"2025-08-27T07:57:09.80374Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:40.324862Z","iopub.status.busy":"2025-08-27T08:43:40.324374Z","iopub.status.idle":"2025-08-27T08:43:42.720549Z","shell.execute_reply":"2025-08-27T08:43:42.719387Z"},"papermill":{"duration":2.407309,"end_time":"2025-08-27T08:43:42.722619","exception":false,"start_time":"2025-08-27T08:43:40.31531","status":"completed"},"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import string\n","import nltk\n","\n","from string import punctuation\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))\n","punctuation = set(string.punctuation)"]},{"cell_type":"code","execution_count":16,"id":"24567398","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:10.795526Z","start_time":"2025-08-27T07:57:10.792088Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:42.740745Z","iopub.status.busy":"2025-08-27T08:43:42.739575Z","iopub.status.idle":"2025-08-27T08:43:42.745585Z","shell.execute_reply":"2025-08-27T08:43:42.74448Z"},"papermill":{"duration":0.016871,"end_time":"2025-08-27T08:43:42.747398","exception":false,"start_time":"2025-08-27T08:43:42.730527","status":"completed"},"tags":[]},"outputs":[],"source":["def filter_tokens(tokens, stop_words, punctuation):\n","    return [word for word in tokens if word.lower() not in stop_words]"]},{"cell_type":"code","execution_count":17,"id":"69dd3e4f","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:10.851374Z","start_time":"2025-08-27T07:57:10.81604Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:42.764853Z","iopub.status.busy":"2025-08-27T08:43:42.764291Z","iopub.status.idle":"2025-08-27T08:43:42.826103Z","shell.execute_reply":"2025-08-27T08:43:42.82508Z"},"papermill":{"duration":0.072299,"end_time":"2025-08-27T08:43:42.827905","exception":false,"start_time":"2025-08-27T08:43:42.755606","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0                                [gremio]\n","1             [good, morrow, neighbour, ]\n","2                              [baptista]\n","3       [good, morrow, neighbour, gremio]\n","4                  [god, save, gentlemen]\n","                      ...                \n","1697                  [match, made, done]\n","1698      [son, shall, daughter, consent]\n","1699                             [tranio]\n","1700             [thank, sir, know, best]\n","1701                           [affied, ]\n","Name: Cleaned_Text, Length: 1702, dtype: object"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["clean_train_tokens1 = tokens_train.apply(lambda tokens: filter_tokens(tokens, stop_words, punctuation))\n","clean_test_tokens1 = tokens_test.apply(lambda tokens: filter_tokens(tokens, stop_words, punctuation))\n","clean_validation_tokens1 = tokens_validation.apply(lambda tokens: filter_tokens(tokens, stop_words, punctuation))\n","\n","clean_validation_tokens1"]},{"cell_type":"code","execution_count":18,"id":"ac4872ae","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:10.994578Z","start_time":"2025-08-27T07:57:10.982882Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:42.845059Z","iopub.status.busy":"2025-08-27T08:43:42.844613Z","iopub.status.idle":"2025-08-27T08:43:42.865094Z","shell.execute_reply":"2025-08-27T08:43:42.864018Z"},"papermill":{"duration":0.031221,"end_time":"2025-08-27T08:43:42.866849","exception":false,"start_time":"2025-08-27T08:43:42.835628","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0                                [gremio]\n","1             [good, morrow, neighbour, ]\n","2                              [baptista]\n","3       [good, morrow, neighbour, gremio]\n","4                  [god, save, gentlemen]\n","                      ...                \n","1697                  [match, made, done]\n","1698      [son, shall, daughter, consent]\n","1699                             [tranio]\n","1700             [thank, sir, know, best]\n","1701                           [affied, ]\n","Name: Cleaned_Text, Length: 1690, dtype: object"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Remove rows where the token list is empty\n","clean_test_tokens1 = clean_test_tokens1[clean_test_tokens1.apply(lambda x: len(x) > 0)]\n","clean_train_tokens1 = clean_train_tokens1[clean_train_tokens1.apply(lambda x: len(x) > 0)]\n","clean_validation_tokens1 = clean_validation_tokens1[clean_validation_tokens1.apply(lambda x: len(x) > 0)]\n","\n","clean_validation_tokens1"]},{"cell_type":"markdown","id":"9da28f2c","metadata":{"papermill":{"duration":0.007308,"end_time":"2025-08-27T08:43:42.881802","exception":false,"start_time":"2025-08-27T08:43:42.874494","status":"completed"},"tags":[]},"source":["This code filters out stopwords and punctuation from tokenized text.<br>\n","\n","- `train_tokens` is a Series where each row contains a list of tokens (words) from the original text.<br>\n","- The `apply` method runs a lambda function on each list of tokens.<br>\n","- For each token, it checks:<br>\n","  - If the lowercase word is **not** in the set of English stopwords (`stop_words`)<br>\n","  - If the word is **not** in the set of punctuation characters (`punctuation`)<br>\n","- Only tokens passing both checks are kept.\n","\n","The result, `clean_t_tokens`, is a Series of lists containing only meaningful words from each row, with stopwords and punctuation removed."]},{"cell_type":"code","execution_count":19,"id":"8e8e7708","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:11.145836Z","start_time":"2025-08-27T07:57:11.142926Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:42.898893Z","iopub.status.busy":"2025-08-27T08:43:42.898485Z","iopub.status.idle":"2025-08-27T08:43:42.904905Z","shell.execute_reply":"2025-08-27T08:43:42.903542Z"},"papermill":{"duration":0.017616,"end_time":"2025-08-27T08:43:42.907062","exception":false,"start_time":"2025-08-27T08:43:42.889446","status":"completed"},"tags":[]},"outputs":[],"source":["def lower_word(tokens):\n","    return [word.lower() for word in tokens]"]},{"cell_type":"code","execution_count":20,"id":"53f974a4","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:11.208397Z","start_time":"2025-08-27T07:57:11.183037Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:42.925333Z","iopub.status.busy":"2025-08-27T08:43:42.924984Z","iopub.status.idle":"2025-08-27T08:43:42.974571Z","shell.execute_reply":"2025-08-27T08:43:42.973434Z"},"papermill":{"duration":0.060568,"end_time":"2025-08-27T08:43:42.976418","exception":false,"start_time":"2025-08-27T08:43:42.91585","status":"completed"},"tags":[]},"outputs":[{"data":{"text/plain":["0                        [first, citizen]\n","1                  [proceed, hear, speak]\n","3                          [speak, speak]\n","4                        [first, citizen]\n","5         [resolved, rather, die, famish]\n","                       ...               \n","29237               [talk, go, sit, weep]\n","29238     [till, find, occasion, revenge]\n","29239                          [baptista]\n","29240    [ever, gentleman, thus, grieved]\n","29241                             [comes]\n","Name: Cleaned_Text, Length: 29062, dtype: object"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Lowercase all tokens in each list\n","clean_train_tokens2 = clean_train_tokens1.apply(lambda tokens: lower_word(tokens))\n","clean_test_tokens2 = clean_test_tokens1.apply(lambda tokens: lower_word(tokens))\n","clean_validation_tokens2 = clean_validation_tokens1.apply(lambda tokens: lower_word(tokens))\n","clean_train_tokens2"]},{"cell_type":"code","execution_count":21,"id":"120ea550","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:13.717569Z","start_time":"2025-08-27T07:57:11.263997Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:43.065542Z","iopub.status.busy":"2025-08-27T08:43:43.064586Z","iopub.status.idle":"2025-08-27T08:43:49.063009Z","shell.execute_reply":"2025-08-27T08:43:49.061824Z"},"papermill":{"duration":6.080804,"end_time":"2025-08-27T08:43:49.064927","exception":false,"start_time":"2025-08-27T08:43:42.984123","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["0                 [first, citizen]\n","1           [proceed, hear, speak]\n","3                   [speak, speak]\n","4                 [first, citizen]\n","5    [resolv, rather, die, famish]\n","Name: Cleaned_Text, dtype: object\n","0                   [first, citizen]\n","1             [proceed, hear, speak]\n","3                     [speak, speak]\n","4                   [first, citizen]\n","5    [resolved, rather, die, famish]\n","Name: Cleaned_Text, dtype: object\n"]}],"source":["from nltk.stem import PorterStemmer, WordNetLemmatizer\n","\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","def stem_and_lemmatize_tokens(token_series, stemmer, lemmatizer):\n","    \"\"\"\n","    Applies stemming and lemmatization to each list of tokens in a pandas Series.\n","\n","    Args:\n","        token_series: pandas Series of lists of tokens.\n","        stemmer: an instance of a stemmer (e.g., PorterStemmer).\n","        lemmatizer: an instance of a lemmatizer (e.g., WordNetLemmatizer).\n","\n","    Returns:\n","        stemmed_tokens: Series of stemmed token lists.\n","        lemmatized_tokens: Series of lemmatized token lists.\n","    \"\"\"\n","    stemmed_tokens = token_series.apply(lambda tokens: [stemmer.stem(word) for word in tokens])\n","    lemmatized_tokens = token_series.apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])\n","    return stemmed_tokens, lemmatized_tokens\n","\n","# Apply stemming and lemmatization to each token in each list\n","stemmed_train, lemmatized_train = stem_and_lemmatize_tokens(clean_train_tokens2, stemmer, lemmatizer)\n","\n","stemmed_test, lemmatized_test = stem_and_lemmatize_tokens(clean_test_tokens2, stemmer, lemmatizer)\n","\n","stemmed_validation, lemmatized_validation = stem_and_lemmatize_tokens(clean_validation_tokens2, stemmer, lemmatizer)\n","\n","print(stemmed_train.head())\n","print(lemmatized_train.head())\n"]},{"cell_type":"code","execution_count":22,"id":"dac76469","metadata":{"ExecuteTime":{"end_time":"2025-08-27T07:57:13.787837Z","start_time":"2025-08-27T07:57:13.783368Z"},"execution":{"iopub.execute_input":"2025-08-27T08:43:49.082735Z","iopub.status.busy":"2025-08-27T08:43:49.081606Z","iopub.status.idle":"2025-08-27T08:43:49.088143Z","shell.execute_reply":"2025-08-27T08:43:49.087056Z"},"papermill":{"duration":0.016949,"end_time":"2025-08-27T08:43:49.089893","exception":false,"start_time":"2025-08-27T08:43:49.072944","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Stemmed Text: befor we proceed ani further, hear me speak.\n","Lemmatized Text: Before we proceed any further, hear me speak.\n"]}],"source":["#In sentences\n","raw_text = \"Before we proceed any further, hear me speak.\"\n","text_stem = \" \".join([stemmer.stem(word) for word in raw_text.split()])\n","text_lemma = \" \".join([lemmatizer.lemmatize(word) for word in raw_text.split()])\n","print(\"Stemmed Text:\", text_stem)\n","print(\"Lemmatized Text:\", text_lemma)"]},{"cell_type":"markdown","id":"f79a1844","metadata":{"papermill":{"duration":0.007608,"end_time":"2025-08-27T08:43:49.105614","exception":false,"start_time":"2025-08-27T08:43:49.098006","status":"completed"},"tags":[]},"source":["AI Overview<br>\n","Tokenizing an NLP dataset row by row is a common and often necessary approach, but the optimal method depends on the specific context and tools being used.<br>\n","Why row-by-row (or batch processing) is common:<br>\n","Individual text units:<br>\n",".\n","Each row in a dataset often represents a distinct unit of text (e.g., a sentence, a document, a social media post) that needs to be processed independently for tasks like sentiment analysis, text classification, or machine translation.<br>\n","Applying a tokenizer:<br>\n",".\n","Tokenizers from libraries like Hugging Face Transformers, NLTK, or spaCy are designed to process individual text inputs or batches of inputs. Applying them row by row ensures each text unit is correctly broken down into tokens according to the chosen tokenization strategy (word, subword, character).<br>\n","Memory management:<br>\n",".\n","Processing large datasets all at once can be memory-intensive. Tokenizing row by row, or in small batches, helps manage memory efficiently, especially when dealing with very long texts or a massive number of rows.<br>\n","Considerations for efficiency:<br>\n","Batching for speed:<br>\n","While \"row by row\" implies individual processing, modern NLP libraries often support batch processing for efficiency. This means you can process multiple rows simultaneously within a single tokenizer call, which significantly speeds up the tokenization of large datasets. Libraries like Hugging Face Datasets and their map function with batched=True are designed for this.<br>\n","Parallel processing:<br>\n","For extremely large datasets, consider using parallel processing techniques (e.g., Python's multiprocessing module, Dask) to distribute the tokenization workload across multiple CPU cores.<br>\n","Pre-trained tokenizers:<br>\n","If using pre-trained models, their associated tokenizers are optimized for efficiency and often handle batching internally.<br>\n","In summary:<br>\n","Tokenizing an NLP dataset involves breaking down text into smaller units (tokens). While the conceptual approach is to process each text unit (often a row), for practical efficiency, especially with large datasets, batching is highly recommended over strictly processing one row at a time. Utilize the batching capabilities of your chosen NLP library or implement parallel processing for optimal performance."]},{"cell_type":"markdown","id":"f43b4d57","metadata":{"papermill":{"duration":0.007787,"end_time":"2025-08-27T08:43:49.121431","exception":false,"start_time":"2025-08-27T08:43:49.113644","status":"completed"},"tags":[]},"source":["AI Overview<br>\n","The decision of whether to remove punctuation before or after tokenization in Python depends on the specific goals of the Natural Language Processing (NLP) task.<br>\n","Removing punctuation before tokenization:<br>\n","Simplifies tokenization:<br>\n","When punctuation is removed beforehand, tokenization becomes more straightforward as the tokenizer primarily focuses on splitting text based on whitespace or other defined delimiters, without needing to handle punctuation as separate tokens or attached to words.<br>\n","Reduces noise and dimensionality:<br>\n","Punctuation marks often carry little semantic meaning for many NLP tasks (e.g., text classification, topic modeling) and can increase the number of unique tokens, leading to higher dimensionality in feature representation. Removing them pre-tokenization can reduce noise and improve model efficiency.<br>\n","Ensures consistent word forms:<br>\n","Words like \"hello\" and \"hello!\" would be treated as distinct tokens if punctuation is not removed, potentially leading to less accurate analysis. Removing punctuation ensures these are treated as the same base word.<br>\n","Removing punctuation after tokenization (or handling it during tokenization):<br>\n","Preserves specific information:<br>\n","In certain NLP tasks, punctuation can be crucial. For example, in sentiment analysis, exclamation marks or question marks can indicate strong emotions. In Named Entity Recognition, hyphens in compound words (e.g., \"state-of-the-art\") might need to be preserved to correctly identify entities.<br>\n","Allows for more nuanced analysis:<br>\n","If the task requires understanding the grammatical structure or stylistic elements of the text, keeping punctuation (or treating it as separate tokens) might be necessary.<br>\n","General Recommendation:<br>\n","For most general-purpose NLP tasks where the focus is on word content rather than specific punctuation nuances, removing punctuation before tokenization is the more common and often recommended approach. This simplifies the data and generally leads to better performance for tasks like text classification, information retrieval, and basic text analysis."]},{"cell_type":"markdown","id":"dea3bcdd","metadata":{"papermill":{"duration":0.007815,"end_time":"2025-08-27T08:43:49.137246","exception":false,"start_time":"2025-08-27T08:43:49.129431","status":"completed"},"tags":[]},"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":4619797,"datasetId":2660745,"isSourceIdPinned":false,"sourceId":4558742,"sourceType":"datasetVersion"}],"dockerImageVersionId":31089,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":88.944752,"end_time":"2025-08-27T08:43:52.718296","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-08-27T08:42:23.773544","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}